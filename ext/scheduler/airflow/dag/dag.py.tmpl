{{- /*gotype: github.com/raystack/optimus/ext/scheduler/airflow/dag.TemplateContext */ -}}
# Code generated by optimus {{.Version}}. DO NOT EDIT.

from datetime import datetime, timedelta

# import Dag level callbacks
from __lib import job_success_event, job_failure_event

# import operator level callbacks
from __lib import operator_start_event, operator_success_event, operator_retry_event, operator_failure_event

from __lib import optimus_sla_miss_notify, SuperKubernetesPodOperator, SuperExternalTaskSensor

from airflow.configuration import conf
from airflow.models import DAG, Variable
from airflow.operators.python_operator import PythonOperator
from airflow.utils.weight_rule import WeightRule
from kubernetes.client import models as k8s

SENSOR_DEFAULT_POKE_INTERVAL_IN_SECS = int(Variable.get("sensor_poke_interval_in_secs", default_var=15 * 60))
SENSOR_DEFAULT_TIMEOUT_IN_SECS = int(Variable.get("sensor_timeout_in_secs", default_var=15 * 60 * 60))
DAG_RETRIES = int(Variable.get("dag_retries", default_var=3))
DAG_RETRY_DELAY = int(Variable.get("dag_retry_delay_in_secs", default_var=5 * 60))
DAGRUN_TIMEOUT_IN_SECS = int(Variable.get("dagrun_timeout_in_secs", default_var=3 * 24 * 60 * 60))
STARTUP_TIMEOUT_IN_SECS = int(Variable.get("startup_timeout_in_secs", default_var=2 * 60))
POOL_SENSOR = Variable.get("sensor_pool", default_var="default_pool")
POOL_TASK = Variable.get("task_pool", default_var="default_pool")
POOL_HOOK = Variable.get("hook_pool", default_var="default_pool")

default_args = {
    "params": {
        "project_name": {{ .Tenant.ProjectName.String | quote}},
        "namespace": {{ .Tenant.NamespaceName.String | quote}},
        "job_name": {{.JobDetails.Name.String | quote}},
        "optimus_hostname": {{.Hostname | quote}}
    },
    {{- if ne .RuntimeConfig.Airflow.Queue "" }}
    "queue": "{{ .RuntimeConfig.Airflow.Queue }}",
    {{- end }}
    "owner": {{.JobDetails.JobMetadata.Owner | quote}},
    "depends_on_past": {{ if .JobDetails.Schedule.DependsOnPast }}True{{- else -}}False{{- end -}},
    "retries": {{ if gt .JobDetails.Retry.Count 0 -}} {{.JobDetails.Retry.Count}} {{- else -}} DAG_RETRIES {{- end}},
    "retry_delay": {{ if gt .JobDetails.Retry.Delay 0 -}} timedelta(seconds={{.JobDetails.Retry.Delay}}) {{- else -}} timedelta(seconds=DAG_RETRY_DELAY) {{- end}},
    "startup_timeout_seconds": STARTUP_TIMEOUT_IN_SECS,
    "retry_exponential_backoff": {{if .JobDetails.Retry.ExponentialBackoff -}}True{{- else -}}False{{- end -}},
    "priority_weight": {{.Priority}},
    "start_date": datetime.strptime({{ .JobDetails.Schedule.StartDate.Format "2006-01-02T15:04:05" | quote }}, "%Y-%m-%dT%H:%M:%S"),
    {{if .JobDetails.Schedule.EndDate -}}
    "end_date": datetime.strptime({{ .JobDetails.Schedule.EndDate.Format "2006-01-02T15:04:05" | quote}}, "%Y-%m-%dT%H:%M:%S"),
    {{- end}}
    "weight_rule": WeightRule.ABSOLUTE,
    {{- if gt .SLAMissDuration 0 }}
    "sla": timedelta(seconds={{ .SLAMissDuration }}),
    {{- end }}
    "on_execute_callback": operator_start_event,
    "on_success_callback": operator_success_event,
    "on_retry_callback"  : operator_retry_event,
    "on_failure_callback": operator_failure_event,
}

{{ if ne .JobDetails.JobMetadata.Description "" -}}
# {{.JobDetails.JobMetadata.Description}}
{{- end }}
dag = DAG(
    dag_id={{.JobDetails.Name.String | quote}},
    default_args=default_args,
    schedule_interval={{ if eq .JobDetails.Schedule.Interval "" }}None{{- else -}} {{ .JobDetails.Schedule.Interval | quote}}{{end}},
    catchup=False,
    dagrun_timeout=timedelta(seconds=DAGRUN_TIMEOUT_IN_SECS),
    tags=[
        {{- range $i, $value := $.JobDetails.GetUniqueLabelValues}}
        "{{ $value }}",
        {{- end}}
    ],
    sla_miss_callback=optimus_sla_miss_notify,
    on_success_callback=job_success_event,
    on_failure_callback=job_failure_event,
)

{{ if .RuntimeConfig.Resource -}}
resources = k8s.V1ResourceRequirements(
{{- if .RuntimeConfig.Resource.Request }}
    requests={
    {{- if ne .RuntimeConfig.Resource.Request.Memory ""}}
        'memory': '{{.RuntimeConfig.Resource.Request.Memory}}',
    {{- end }}
    {{- if ne .RuntimeConfig.Resource.Request.CPU ""}}
        'cpu': '{{.RuntimeConfig.Resource.Request.CPU}}',
    {{- end }}
    },
{{- end }}
{{- if .RuntimeConfig.Resource.Limit }}
    limits={
    {{- if ne .RuntimeConfig.Resource.Limit.Memory ""}}
        'memory': '{{.RuntimeConfig.Resource.Limit.Memory}}',
    {{- end }}
    {{- if ne .RuntimeConfig.Resource.Limit.CPU ""}}
        'cpu': '{{.RuntimeConfig.Resource.Limit.CPU}}',
    {{- end }}
    },
{{ end -}}
)
{{- end }}

JOB_DIR = "/data"
IMAGE_PULL_POLICY = "IfNotPresent"
INIT_CONTAINER_IMAGE = "raystack/optimus:{{.Version}}"
INIT_CONTAINER_ENTRYPOINT = "/opt/entrypoint_init_container.sh"

def get_entrypoint_cmd(plugin_entrypoint_script):
    path_config = JOB_DIR + "/in/.env"
    path_secret = JOB_DIR + "/in/.secret"
    entrypoint = "set -o allexport; source {path_config}; set +o allexport; cat {path_config}; ".format(path_config=path_config)
    entrypoint += "set -o allexport; source {path_secret}; set +o allexport; ".format(path_secret=path_secret)
    return entrypoint + plugin_entrypoint_script

volume = k8s.V1Volume(
    name='asset-volume',
    empty_dir=k8s.V1EmptyDirVolumeSource()
)

asset_volume_mounts = [
    k8s.V1VolumeMount(mount_path=JOB_DIR, name='asset-volume', sub_path=None, read_only=False)
]

executor_env_vars = [
    k8s.V1EnvVar(name="JOB_LABELS", value='{{.JobDetails.GetLabelsAsString}}'),
    k8s.V1EnvVar(name="JOB_DIR", value=JOB_DIR),
    k8s.V1EnvVar(name="JOB_NAME", value='{{$.JobDetails.Name}}'),
]

init_env_vars = [
    k8s.V1EnvVar(name="JOB_DIR", value=JOB_DIR),
    k8s.V1EnvVar(name="JOB_NAME", value='{{$.JobDetails.Name}}'),
    k8s.V1EnvVar(name="OPTIMUS_HOST", value='{{$.Hostname}}'),
    k8s.V1EnvVar(name="PROJECT", value='{{$.Tenant.ProjectName.String}}'),
    k8s.V1EnvVar(name="SCHEDULED_AT", value='{{ "{{ next_execution_date }}" }}'),
]

init_container = k8s.V1Container(
    name="init-container",
    image=INIT_CONTAINER_IMAGE,
    image_pull_policy=IMAGE_PULL_POLICY,
    env=init_env_vars + [
        k8s.V1EnvVar(name="INSTANCE_TYPE", value='{{$.ExecutorTask}}'),
        k8s.V1EnvVar(name="INSTANCE_NAME", value='{{.Task.Name}}'),
    ],
    security_context=k8s.V1PodSecurityContext(run_as_user=0),
    volume_mounts=asset_volume_mounts,
    command=["/bin/sh", INIT_CONTAINER_ENTRYPOINT],
)

{{ $transformationName := print "transformation_"  .Task.Name | DisplayName -}}
{{$transformationName}} = SuperKubernetesPodOperator(
    image_pull_policy=IMAGE_PULL_POLICY,
    namespace=conf.get('kubernetes', 'namespace', fallback="default"),
    image={{  .Task.Image | quote}},
    cmds=["{{.Task.Entrypoint.Shell}}", "-c"],
    arguments=[get_entrypoint_cmd("""{{.Task.Entrypoint.Script}} """)],
    name="{{  .Task.Name | replace "_" "-" }}",
    task_id={{ .Task.Name | quote}},
    get_logs=True,
    dag=dag,
    depends_on_past={{ if .JobDetails.Schedule.DependsOnPast }}True{{- else -}}False{{- end -}},
    in_cluster=True,
    is_delete_operator_pod=True,
    do_xcom_push=False,
    env_vars=executor_env_vars,
    {{- if .RuntimeConfig.Resource }}
    resources=resources,
    {{- end }}
    reattach_on_restart=True,
    volume_mounts=asset_volume_mounts,
    volumes=[volume],
    init_containers=[init_container],
    pool={{ if eq .RuntimeConfig.Airflow.Pool "" }}POOL_TASK{{- else -}} {{ .RuntimeConfig.Airflow.Pool | quote}}{{end}}
)

# hooks loop start
{{- range $_, $t := .Hooks.List }}
{{- $hookName := $t.Name | ReplaceDash }}
init_container_{{$hookName}} = k8s.V1Container(
    name="init-container",
    image=INIT_CONTAINER_IMAGE,
    image_pull_policy=IMAGE_PULL_POLICY,
    env=init_env_vars + [
        k8s.V1EnvVar(name="INSTANCE_TYPE", value='{{$.ExecutorHook}}'),
        k8s.V1EnvVar(name="INSTANCE_NAME", value='{{$hookName}}'),
    ],
    security_context=k8s.V1PodSecurityContext(run_as_user=0),
    volume_mounts=asset_volume_mounts,
    command=["/bin/sh", INIT_CONTAINER_ENTRYPOINT],
)

hook_{{$hookName}} = SuperKubernetesPodOperator(
    image_pull_policy=IMAGE_PULL_POLICY,
    namespace=conf.get('kubernetes', 'namespace', fallback="default"),
    image="{{ $t.Image }}",
    cmds=["{{$t.Entrypoint.Shell}}", "-c"],
    arguments=[get_entrypoint_cmd("""{{$t.Entrypoint.Script}} """)],
    name="hook_{{ $t.Name | replace "_" "-" }}",
    task_id="hook_{{ $t.Name }}",
    get_logs=True,
    dag=dag,
    in_cluster=True,
    depends_on_past=False,
    is_delete_operator_pod=True,
    do_xcom_push=False,
    env_vars=executor_env_vars,
    {{- if $t.IsFailHook }}
    trigger_rule="one_failed",
    {{- end }}
    {{- if $.RuntimeConfig.Resource }}
    resources=resources,
    {{- end }}
    reattach_on_restart=True,
    volume_mounts=asset_volume_mounts,
    volumes=[volume],
    init_containers=[init_container_{{$hookName}}],
    pool={{ if eq $.RuntimeConfig.Airflow.Pool "" }}POOL_HOOK{{- else -}} {{ $.RuntimeConfig.Airflow.Pool | quote}}{{end}}
)
{{- end }}
# hooks loop ends


# create upstream sensors
{{- $baseWindow := $.JobDetails.Job.Window }}
{{- range $_, $upstream := .Upstreams.Upstreams}}
{{- $dependencyName := $upstream.JobName | DisplayName }}
wait_{{ $dependencyName }} = SuperExternalTaskSensor(
    optimus_hostname="{{$.Hostname}}",
    upstream_optimus_hostname="{{$upstream.Host}}",
    upstream_optimus_project="{{$upstream.Tenant.ProjectName.String}}",
    upstream_optimus_namespace="{{$upstream.Tenant.NamespaceName.String}}",
    upstream_optimus_job="{{$upstream.JobName}}",
    window_size="{{ $baseWindow.GetSize }}",
    window_version=int("{{ $baseWindow.GetVersion }}"),
    poke_interval=SENSOR_DEFAULT_POKE_INTERVAL_IN_SECS,
    timeout=SENSOR_DEFAULT_TIMEOUT_IN_SECS,
    task_id="wait_{{$upstream.JobName}}-{{$upstream.TaskName}}",
    depends_on_past=False,
    dag=dag,
    pool={{ if eq $.RuntimeConfig.Airflow.Pool "" }}POOL_SENSOR{{- else -}} {{ $.RuntimeConfig.Airflow.Pool | quote}}{{end}}
)
{{ end}}

{{- range $_, $httpUpstream := $.Upstreams.HTTP}}
headers_dict_{{$httpUpstream.Name}} = { {{- range $k, $v := $httpUpstream.Headers}} '{{$k}}': '{{$v}}', {{- end}} }
request_params_dict_{{$httpUpstream.Name}} = { {{- range $key, $value := $httpUpstream.Params}} '{{$key}}': '{{$value}}', {{- end}} }

wait_{{$httpUpstream.Name}} = ExternalHttpSensor(
    endpoint='{{$httpUpstream.URL}}',
    headers=headers_dict_{{$httpUpstream.Name}},
    request_params=request_params_dict_{{$httpUpstream.Name}},
    poke_interval=SENSOR_DEFAULT_POKE_INTERVAL_IN_SECS,
    timeout=SENSOR_DEFAULT_TIMEOUT_IN_SECS,
    task_id='wait_{{$httpUpstream.Name}}',
    depends_on_past=False,
    dag=dag,
    pool={{ if eq $.RuntimeConfig.Airflow.Pool "" }}POOL_SENSOR{{- else -}} {{ $.RuntimeConfig.Airflow.Pool | quote}}{{end}}
)
{{- end -}}

# arrange inter task dependencies
####################################

# upstream sensors -> base transformation task
{{- range $i, $t := $.Upstreams.Upstreams }}
wait_{{ $t.JobName | DisplayName }} >> {{$transformationName}}
{{- end}}
{{- range $_, $t := $.Upstreams.HTTP }}
wait_{{ $t.Name }} >> {{$transformationName}}
{{- end}}

# setup hooks and dependencies
# [Dependency/HttpDep/ExternalDep/PreHook] -> Task -> [Post Hook -> Fail Hook]

# setup hook dependencies
{{- range $_, $h := .Hooks.Pre }}
hook_{{$h.Name | ReplaceDash}} >> {{$transformationName}}
{{- end }}

{{$transformationName}}
{{- if .Hooks.Post }} >> [
    {{- range $_, $h := .Hooks.Post -}}
        hook_{{$h.Name | ReplaceDash}},
    {{- end -}} ]
{{- end -}}
{{- if .Hooks.Fail }} >> [
    {{- range $_, $h := .Hooks.Fail -}}
       hook_{{$h.Name | ReplaceDash}},
    {{- end -}} ]
{{- end }}

# set inter-dependencies between hooks and hooks
{{- range $before, $after := .Hooks.Dependencies }}
hook_{{$before | ReplaceDash}} >> hook_{{$after | ReplaceDash}}
{{- end }}
