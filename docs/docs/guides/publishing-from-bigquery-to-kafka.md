---
id: publishing-bigquery-to-kafka
title: Publishing from bigQuery to kafka
---

Following are the steps to publish BQ data to Kafka:

1. Raise a ticket with DE to create the proto, soon this will be automated.
2. As part of Job Specification answer the questions related to pushing data to Kafka.
3. Commit and Push

Publishing from Bigquery to Kafka is done using an Optimus hook called `Transporter`. 
Once data is pushed to Kafka you can use firehose to consume messages for your needs. 
In order to add hook to an existing Job, run the following command and answer the 
corresponding prompts:

```
$ ./optimus job addhook
? Select a Job example_job
? Which hook to run? transporter
? Filter expression for extracting transformation rows? event_timestamp >= '{{.DSTART}}' AND event_timestamp < '{{.DEND}}'
```
Note: this is not available for public use at the moment

### Config:

- Filter expression: Expression is used as a where clause to restrict the number of rows to only push the ones 
that are affected by current transformation. Expression can be templated with DSTART and DEND macros which will be replaced with the window for which the current transformation is getting executed similar to what we do in Job specifications.

After this, existing job.yaml file will get updated with the new hooks config and the job specification would look like below:

```yaml
version: 1
name: example_job
owner: example@example.com
schedule:
  start_date: "2021-02-18"
  interval: 0 3 * * *
behavior:
  depends_on_past: false
  catch_up: true
task:
  name: bq2bq
  config:
    DATASET: data
    LOAD_METHOD: APPEND
    PROJECT: example
    SQL_TYPE: STANDARD
    TABLE: hello_table
  window:
    size: 24h
    offset: "0"
    truncate_to: d
dependencies: []
hooks:
- name: transporter
  config:
    BQ_DATASET: '{{.TASK__DATASET}}' # inherited from task configs
    BQ_PROJECT: '{{.TASK__PROJECT}}'
    BQ_TABLE: '{{.TASK__TABLE}}'
    FILTER_EXPRESSION: 'event_timestamp >= "{{.DSTART}}" AND event_timestamp < "{{.DEND}}"'
    KAFKA_TOPIC: optimus_example-data-hello_table
    PRODUCER_CONFIG_BOOTSTRAP_SERVERS: '{{.GLOBAL__TRANSPORTER_KAFKA_BROKERS}}'
    PROTO_SCHEMA: example.data.HelloTable
    STENCIL_URL: '{{.GLOBAL__TRANSPORTER_KAFKA_BROKERS}}' # will be defined as global config
```

Some configuration would be auto-generated by Optimus. 
You can now commit and push the changes in Job specification.

There are standards that we want users to maintain but haven’t enforced:

1. Single table to topic mapping to be maintained & following a naming convention 
   helps in better discoverability. Topic names are auto populated with the same naming convention.
2. Kafka Brokers are not modifiable & pre-selected per every entity.
3. All Protos & Stencil are managed & doesn’t need any user intervention.

### Bigquery and Protobuf Data Type Mapping

| Bigquery Type | Protobuf Type             |
|---------------|---------------------------|
| STRING        | string                    |
| BYTES         | bytes                     |
| INTEGER       | int64                     |
| FLOAT         | float                     |
| BOOLEAN       | bool                      |
| TIMESTAMP     | google.protobuf.Timestamp |
| DATE          | string                    |
| TIME          | string                    |
| DATETIME      | string                    |
| NUMERIC       | float                     |
| GEOGRAPHY     | string                    |
