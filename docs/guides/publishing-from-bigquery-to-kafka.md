# Publishing from BigQuery to Kafka

Following are the steps to publish BQ data to Kafka:

1. Raise a ticket with DE to create the proto, soon this will be automated.
2. As part of Job Specification answer the questions related to pushing data to Kafka.
3. Commit and Push

Publishing from Bigquery to Kafka is done using an Optimus hook called `Transporter`. Once data is pushed to Kafka you can use firehose to consume messages for your needs. In order to add hook to an existing Job, run the following command and answer the corresponding prompts:

```
$ ./opctl create hook
? Select a Job example_job
? Which hook to run? transporter
? Where should the hook run with respect to task? post
? Filter expression for extracting transformation rows? event_timestamp >= '{{.DSTART}}' AND event_timestamp < '{{.DEND}}'
```

Below specified details need to be provided:

**Filter expression** : Expression is used as a where clause to restrict the number of rows to only push the ones that are affected by current transformation. Expression can be templated with DSTART and DEND macros which will be replaced with the window for which the current transformation is getting executed similar to what we do in Job specifications.

After this, existing job.yaml file will get updated with the new hooks config and the job specification would look like below:

```yaml
version: 1
name: example_job
owner: example@opctl.com
schedule:
  start_date: "2021-02-18"
  interval: 0 3 * * *
behavior:
  depends_on_past: false
  catch_up: true
task:
  name: bq2bq
  config:
    DATASET: data
    JOB_LABELS: owner=optimus
    LOAD_METHOD: APPEND
    PROJECT: example
    SQL_TYPE: STANDARD
    TABLE: hello_table
    TASK_TIMEZONE: UTC
  window:
    size: 24h
    offset: "0"
    truncate_to: d
dependencies: []
hooks:
- name: transporter
  type: post
  config:
    BQ_DATASET: data
    BQ_PROJECT: example
    BQ_TABLE: hello_table
    FILTER_EXPRESSION: event_timestamp >= '{{.DSTART}}' AND event_timestamp < '{{.DEND}}'
    JOB_LABELS: owner=optimus
    KAFKA_TOPIC: optimus_example-data-hello_table
    PRODUCER_CONFIG_BOOTSTRAP_SERVERS: '{{.transporterKafkaBroker}}'
    PROTO_SCHEMA: example.data.HelloTable
    STENCIL_URL: http://artifactory.url.io/
```

Some of the configuration would be auto-generated by Optimus. You can now commit and push the changes in Job specification.

There are standards that we want users to maintain haven’t enforced all:

1. Single table to topic mapping to be maintained & following a naming convention helps in better discoverability. Topic names are auto populated with the same naming convention.
2. Kafka Brokers are not modifiable & pre-selected per every entity.
3. All Protos & Stencil are managed & doesn’t need any user intervention.

### Bigquery and Protobuf Data Type Mapping

| Bigquery Type | Protobuf Type             |
|---------------|---------------------------|
| STRING        | string                    |
| BYTES         | bytes                     |
| INTEGER       | int64                     |
| FLOAT         | float                     |
| BOOLEAN       | bool                      |
| TIMESTAMP     | google.protobuf.Timestamp |
| DATE          | string                    |
| TIME          | string                    |
| DATETIME      | string                    |
| NUMERIC       | float                     |
| GEOGRAPHY     | string                    |
